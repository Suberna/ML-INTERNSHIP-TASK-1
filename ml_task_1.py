# -*- coding: utf-8 -*-
"""ML TASK 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17kIq3LDJLEoUCv9r5CVfd_IqDSt-sLke

LOAD TRAIN DATASET
"""

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer
import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
import re
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report

train_data_path = "/content/train_data.txt"
train_data = pd.read_csv("/content/train_data.txt",header =None,sep=":::",names =["ID","Title","Genres","Description"],engine ="python")

train_data.head()

"""To check whether everything is filled"""

train_data.info()

train_data.isnull().sum()

"""Filled the missing value"""

train_data["Title"].fillna("Unknown",inplace =True)
mode_genre =train_data['Genres'].mode()[0]
train_data["Genres"].fillna(mode_genre,inplace=True)
train_data["Description"].fillna("No description available",inplace =True)

train_data.isnull().sum()

"""LOAD TEST DATASET"""

test_path ="/content/test_data.txt"
test_data = pd.read_csv(test_path,sep =":::",names=["ID","Title","Description"],engine="python")

test_data.head()

test_data.info()

test_data.isnull().sum()

"""DATA PREPROCESSING"""

# Initialize the stemmer and stop words
stemmer = LancasterStemmer()
stop_words = set(stopwords.words('english'))

# Define the clean_text function
def clean_text(text):
    text = text.lower()  # Lowercase all characters
    text = re.sub(r'@\S+', '', text)  # Remove Twitter handles
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'pic.\S+', '', text)
    text = re.sub(r"[^a-zA-Z+']", ' ', text)  # Keep only characters
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text + ' ')  # Keep words with length > 1 only
    text = "".join([i for i in text if i not in string.punctuation])
    words = nltk.word_tokenize(text)
    stopwords = nltk.corpus.stopwords.words('english')  # Remove stopwords
    text = " ".join([i for i in words if i not in stopwords and len(i) > 2])
    text = re.sub("\s[\s]+", " ", text).strip()  # Remove repeated/leading/trailing spaces
    return text

# Apply the clean_text function to the 'Description' column in the training and test data
train_data['Text_cleaning'] = train_data['Description'].apply(clean_text)
test_data['Text_cleaning'] = test_data['Description'].apply(clean_text)

# Droping the redundant data
print("shape before drop nulls",train_data.shape)
train_data = train_data.drop_duplicates()
print("shape after drop nulls",train_data.shape)

import warnings
warnings.filterwarnings("ignore", "use_inf_as_na")

# Calculate the length of cleaned text
train_data['length_Text_cleaning'] = train_data['Text_cleaning'].apply(len)
# Visualize the distribution of text lengths
plt.figure(figsize=(8, 7))
sns.histplot(data=train_data, x='length_Text_cleaning', bins=20, kde=True, color='blue')
plt.xlabel('Length', fontsize=14, fontweight='bold')
plt.ylabel('Frequency', fontsize=14, fontweight='bold')
plt.title('Distribution of Lengths', fontsize=16, fontweight='bold')
plt.show()

# Create histograms to visualize the distribution of text lengths before and after cleaning

# Set up the figure with two subplots
plt.figure(figsize=(12, 6))

# Subplot 1: Original text length distribution
plt.subplot(1, 2, 1)
original_lengths = train_data['Description'].apply(len)
plt.hist(original_lengths, bins=range(0, max(original_lengths) + 100, 100), color='blue', alpha=0.7)
plt.title('Original Text Length')
plt.xlabel('Text Length')
plt.ylabel('Frequency')

# Subplot 2: Cleaned text length distribution
plt.subplot(1, 2, 2)
cleaned_lengths = train_data['Text_cleaning'].apply(len)
plt.hist(cleaned_lengths, bins=range(0, max(cleaned_lengths) + 100, 100), color='green', alpha=0.7)
plt.title('Cleaned Text Length')
plt.xlabel('Text Length')
plt.ylabel('Frequency')

# Adjust layout and display the plots
plt.tight_layout()
plt.show()

"""TEXT VECTORIZATION"""

#Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()
#Fit and transform the training data
X_train = tfidf_vectorizer.fit_transform(train_data["Text_cleaning"])
#Transform the test data
X_test = tfidf_vectorizer.transform(test_data["Text_cleaning"])

"""SPLIT DATA AND TRAIN MODEL"""

#Splitting data into training and validation sets
X = X_train
y = train_data['Genres']
X_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.2,random_state =42)

"""NAIVE BAYES"""

import warnings
warnings.filterwarnings("ignore")
#Initializing and training Multinomial Navie Bayes Classifier
clf_NB = MultinomialNB()
clf_NB.fit(X_train,y_train)
#Making predictions
y_pred = clf_NB.predict(X_val)
#Evaluate performance of the model
accuracy = accuracy_score(y_val,y_pred)
print("Validdation Accuracy:",accuracy)
print(classification_report(y_val,y_pred))

accuracy_NB = accuracy_score(y_val,y_pred)
accuracy_NB

"""MAKING PREDICTION"""

#Using trained model to predict on the test data
X_test_predictions = clf_NB.predict(X_test)
test_data["Predicted_Genres_NB"] = X_test_predictions

test_data.head()

